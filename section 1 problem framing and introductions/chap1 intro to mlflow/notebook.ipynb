{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 : Introducing MLflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLflow** is an open source platform for the **machine learning (ML)** life cycle, with a focus on *reproducibility*, *training*, and *deployment*. It is based on an open interface design and is able to work with any language or platform, with clients in Python and Java, and is accessible through a REST API. Scalability is also an important benefit that an ML developer can leverage with MLflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is MLflow?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a product based on ML can be a laborious task. There is a general need to reduce the friction between different steps of the ML development life cycle, and between teams of data scientists and engineers that are involved in the process. ML practitioners, such as data scientists and ML engineers, operate with different systems, standards, and tools. While data scientists spend most of their time developing models in tools such as Jupyter Notebooks, when running in production, the model is deployed in the context of a software application with an environment that is more demanding in terms of scale and reliability.\n",
    "\n",
    "A common occurrence in ML projects is to have **the models reimplemented by an engineering team**, **creating a custom-made system to serve the specific model**. A set of challenges are common with teams that follow bespoke approaches regarding model development:\n",
    "\n",
    "* ML projects that run over budget due to the need to create bespoke software infrastructure to develop and serve models\n",
    "* Translation errors when reimplementing the models produced by data scientists\n",
    "* Scalability issues when serving predictions\n",
    "* Friction in terms of reproducing training processes between data scientists due to a lack of standard environments\n",
    "\n",
    "Companies leveraging ML tend to create their own (often extremely laborious) internal systems in order to ensure a smooth and structured process of ML development. Widely documented ML platforms include systems such as Michelangelo and FBLearner, from Uber and Facebook, respectively.\n",
    "\n",
    "It is in the context of the increasing adoption of ML that MLflow was initially created at Databricks and open sourced as a platform, to aid in the implementation of ML systems.\n",
    "\n",
    "MLflow enables an everyday practitioner in one platform to manage the ML life cycle, from iteration on model development up to deployment in a reliable and scalable environment that is compatible with modern software system requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting started with MLflow**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a Dockerfile with the following instructions\n",
    "```dockerfile\n",
    "FROM jupyter/datascience-notebook\n",
    "RUN pip install mlflow\n",
    "RUN pip install sklearn\n",
    "```\n",
    "2. Create a running file `run.sh` with the following instructions\n",
    "```bash\n",
    "#!/bin/bash\n",
    "docker build -t chapter_1_homlflow .\n",
    "docker run -p 8888:8888 -p 5000:5000 -v $(pwd):/home/ibra/ -it chapter_1_homlflow\n",
    "```\n",
    "3. Open your browser to `http://localhost:8888` and you should be able to navigate to the Chapter01 folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Developping your first model with MLflow**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample dataset:\n",
    "```py\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.4)\n",
    "```\n",
    "Next, let's train your model.\n",
    "Training a simple machine model with a framework such as scikit-learn involves instantiating an estimator such as LogisticRegression and calling the fit command to execute training over the Iris dataset built in scikit-learn:\n",
    "```py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "The preceding lines of code are just a small portion of the ML Engineering process. As will be demonstrated, a non-trivial amount of code needs to be created in order to productionize and make sure that the preceding training code is usable and reliable. One of the main objectives of MLflow is to aid in the process of setting up ML systems and projects. In the following sections, we will demonstrate how MLflow can be used to make your solutions robust and reliable.\n",
    "\n",
    "Then, we will add MLflow.\n",
    "With a few more lines of code, you should be able to start your first MLflow interaction. In the following code listing, we start by importing the mlflow module, followed by the LogisticRegression class in scikit-learn. You can use the accompanying Jupyter notebook to run the next section:\n",
    "```py\n",
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "```\n",
    "The `mlflow.sklearn.autolog()` instruction enables you to automatically log the experiment in the local directory. It captures the metrics produced by the underlying ML library in use. MLflow Tracking is the module responsible for handling metrics and logs. By default, the metadata of an MLflow run is stored in the local filesystem.\n",
    "\n",
    "If you run the following excerpt on the accompanying notebook's root document, you should now have the following files in your home directory as a result of running the following command:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.pkl` file contains a **serialized version of the model**. For a scikit-learn model, there is a binary version of the Python code of the model. Upon autologging, the metrics are leveraged from the underlying machine library in use. The default packaging strategy was based on a conda.yaml file, with the right dependencies to be able to serialize the model.\n",
    "\n",
    "The `MLmodel` file **is the main definition of the project from an MLflow project** with information related to how to run inference on the current model.\n",
    "\n",
    "The `metrics` folder contains **the training score value of this particular run of the training process**, which can be used to benchmark the model with further model improvements down the line.\n",
    "\n",
    "The `params` folder on the first listing of folders contains **the default parameters of the logistic regression model**, with the different default possibilities listed transparently and stored automatically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow modules**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow modules are software components that deliver the core features that aid in the different phases of the ML life cycle. MLflow features are delivered through modules, extensible components that organize related features in the platform.\n",
    "\n",
    "The following are the built-in modules in MLflow:\n",
    "\n",
    "* **MLflow Tracking**: Provides a mechanism and UI to handle metrics and artifacts generated by ML executions (training and inference)\n",
    "* **Mlflow Projects**: A package format to standardize ML projects\n",
    "Mlflow Models: A mechanism that deploys to different types of environments, both on-premises and in the cloud\n",
    "* **Mlflow Model Registry**: A module that handles the management of models in MLflow and its life cycle, including state\n",
    "\n",
    "In order to explore the different modules, we will install MLflow in your local environment using the following command:\n",
    "```bash\n",
    "pip install mlflow\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow projects**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLflow project represents the basic unit of organization of ML projects. There are three different environments supported by MLflow projects: the **Conda environment**, **Docker**, and the **local system**.\n",
    "Once you have your environment, the main file that defines how your project should look is the **MLProject** file. This file is used by MLflow to understand how it should run your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Developping your first e2e pipeline**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prototype a **simple stock prediction project** in this section with MLflow and will document the different files and phases of the solution. You will develop it **in your local system using the MLflow and Docker installed locally**.\n",
    "\n",
    "> In this section, we are assuming that MLflow and Docker are installed locally, as the steps in this section will be executed in your local environment.\n",
    "\n",
    "The task in this illustrative project is to create a basic MLflow project and produce a working baseline ML model to predict, based on market signals over a certain number of days, whether the stock market will go up or down.\n",
    "\n",
    "In this section, we will use a Yahoo Finance dataset available for quoting the BTC-USD pair in https://finance.yahoo.com/quote/BTC-USD/ over a period of 3 months. We will train a model to predict whether the quote will be going up or not on a given day. A REST API will be made available for predictions through MLflow.\n",
    "\n",
    "We will illustrate, step by step, **the creation of an MLflow project to train a classifier on stock data**, using the Yahoo API for financial information retrieved using the package's pandas data reader:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add your MLProject file:\n",
    "\n",
    "```Yaml\n",
    "name: stockpred\n",
    "docker_env:\n",
    "  image: stockpredictor-docker\n",
    "entry_points:\n",
    "  main:\n",
    "    command: \"python train.py\"\n",
    "```\n",
    "\n",
    "  The preceding **MLProject** file specifies that **dependencies will be managed in Docker with a specific image name**. **MLflow will try to pull the image using the version of Docker installed on your system**. If it doesn't find it, it will try to retrieve it from Docker Hub. For the goals of this chapter, it is completely fine to have MLflow running on your local machine.\n",
    "\n",
    "  The second configuration that we add to our project **is the main entry point command**. The command to be executed will invoke in the Docker environment the `train.py` Python file, which contains the code of our project.\n",
    "\n",
    "2. Add a Docker file to the project.\n",
    "\n",
    "   Additionally, you can specify the Docker registry URL of your image. The advantage of running Docker is that your project is not bound to the Python language, as we will see in the advanced section of this book. The MLflow API is available in a Rest interface alongside the official clients: Python, Java, and R:\n",
    "\n",
    "```dockerfile\n",
    "FROM continuumio/miniconda:4.12.0\n",
    "\n",
    "RUN pip install mlflow \\\n",
    "    && pip install numpy \\\n",
    "    && pip install scipy \\\n",
    "    && pip install pandas \\\n",
    "    && pip install scikit-learn \\\n",
    "    && pip install cloudpickle \\\n",
    "    && pip install pandas-datareader>=0.10.0\n",
    "```\n",
    "The preceding Docker image file is based on the open source package Miniconda, a free minimal installer with a minimal set of packages for data science that allow us to control the details of the packages that we need in our environment.\n",
    "\n",
    "We will specify the version of MLflow (our ML platform), `numpy`, and `scipy` for numerical calculations. `Cloudpickle` allows us to easily serialize objects. We will use pandas to manage data frames, and `pandas_datareader` to allow us to easily retrieve the data from public sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Import the packages required for the project.\n",
    "\n",
    "On the following listing, we explicitly import all the libraries that we will use during the execution of the training script: the library to read the data, and the different sklearn modules related to the chosen initial ML model:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow.sklearn\n",
    "```\n",
    "We explicitly chose for the stock market movement detection problem a `RandomForestClassifier`, due to the fact that **it's an extremely versatile and widely accepted baseline model for classification problems**.\n",
    "\n",
    "4. Acquire your training data.\n",
    "\n",
    "The component of the code that acquires the Yahoo Finance stock dataset is intentionally small, so we choose a specific interval of 3 months to train our classifier.\n",
    "The `acquire_training_data` method returns a pandas data frame with the relevant dataset:\n",
    "\n",
    "```python\n",
    "def acquire_training_data():\n",
    "    start = datetime.datetime(2022, 9, 1)\n",
    "    end = datetime.datetime(2022, 11, 30)\n",
    "    df = web.DataReader(\"BTC-USD\", 'yahoo', start, end)\n",
    "    return df\n",
    "```\n",
    "5. Make the data usable by scikit-learn.\n",
    "\n",
    "The data acquired in the preceding step is clearly not directly usable by `RandomForestAlgorithm`, which thrives on categorical features. In order to facilitate the execution of this, we will transform the raw data into a feature vector using the rolling window technique.\n",
    "\n",
    "Basically, **the feature vector for each day becomes the deltas between the current and previous window days**. In this case, we use the previous day's market movement (`1 for a stock going up`, `0 otherwise`):\n",
    "\n",
    "```python\n",
    "def digitize(n):\n",
    "    if n > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    \"\"\"\n",
    "        Takes np.array 'a' and size 'window' as parameters\n",
    "        Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'\n",
    "        e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )\n",
    "             Output:\n",
    "                     array([[1, 2, 3, 4],\n",
    "                           [2, 3, 4, 5],\n",
    "                           [3, 4, 5, 6]])\n",
    "    \"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    data['Delta'] = data['Close'] - data['Open']\n",
    "    data['to_predict'] = data['Delta'].apply(lambda d: digitize(d))\n",
    "    return data\n",
    "```\n",
    "\n",
    "6. Train and store your model in MLflow.\n",
    "\n",
    "This portion of the following code listing calls the data preparation methods declared previously and executes the prediction process.\n",
    "\n",
    "The main execution also explicitly logs the ML model trained in the current execution in the MLflow environment\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    with mlflow.start_run():\n",
    "        training_data = acquire_training_dataset()\n",
    "        prepared_training_data_df = prepare_training_data(training_data)\n",
    "        btc_mat = prepared_training_data_df.as_matrix()\n",
    "        WINDOW_SIZE = 14\n",
    "        X = rolling_window(btc_mat[:, 7], WINDOW_SIZE[:-1, :])\n",
    "        Y =  prepared_training_data_df['to_predict'].as_matrix()[WINDOW_SIZE:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 4284, stratify=Y)\n",
    "        clf = RandomForestClassifier(bootstrap=True, criterion='gini', mini_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, random_state=4284, verbose=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predicted = clf.predict(X_test)\n",
    "        mlflow.sklearn.log_model(clf, \"model_random_forest\")\n",
    "        mlflow.log_metric(\"precision_label_0\", precision_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"recall_label_0\", recall_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"f1score_label_0\", f1_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"precision_label_1\", precision_score(y_test, predicted, pos_label=1))\n",
    "        mlflow.log_metric(\"recall_label_1\", recall_score(y_test, predicted, pos_label=1))\n",
    "        mlflow.log_metric(\"f1score_label_1\", f1_score(y_test, predicted, pos_label=1))\n",
    "```\n",
    "\n",
    "The `mlflow.sklearn.log_model(clf, \"model_random_forest\")` method **takes care of persisting the model upon training**. In contrast to the previous example, **we are explicitly asking MLflow to log the model and the metrics that we find relevant**. **This flexibility in the items to log allows one program to log multiple models into MLflow**.\n",
    "\n",
    "7. Build your project's Docker image.\n",
    "\n",
    "In order to build your Docker image, you should run the following command:\n",
    "```bash\n",
    "docker build -t stockpred -f dockerfile .\n",
    "```\n",
    "This will build the image specified previously with the stockpred tag. This image will be usable in MLflow in the subsequent steps as the model is now logged into your local registry.\n",
    "\n",
    "1. Run your project.\n",
    "In order to run your project, you can now run the MLflow project:\n",
    "```bash\n",
    "mlflow run .\n",
    "```\n",
    "At this stage, you have **a simple, reproducible baseline of a stock predictor pipeline using MLflow** that you can improve on and easily share with others."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow tracking**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MLflow tracking** component is responsible for observability. The main features of this module are the logging of metrics, artifacts, and parameters of an MLflow execution. It provides vizualisations and artifact management features.\n",
    "\n",
    "In a **production setting, it is used as a centralized tracking server implemented in Python that can be shared by a group of ML practitioners in an organization**. This **enables improvements in ML models to be shared within the organization**.\n",
    "\n",
    "you can run the following command to have access to the results of your runs:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLflow Models** is the core component that handles the different model flavors that are supported in MLflow and intermediates the deployment into different execution environments.\n",
    "\n",
    "We will now delve into the different models supported in the latest version of MLflow.\n",
    "\n",
    "As shown in the *Getting started with MLflow* section, MLflow models have a specific serialization approach for when the model is persisted in its internal format. For example, the serialized folder of the model implemented on the **stockpredictor** project would look like the following:\n",
    "```bash\n",
    "- MLmodel\n",
    "- conda.yaml\n",
    "- model.pkl\n",
    "```\n",
    "\n",
    "MLflow, by default, supports serving models in two flavors, namely, as a **python_function** or in **sklearn** format. The flavors are basically a format to be used by tools or environments serving models.\n",
    "\n",
    "A good example of using the preceding is being able to serve your model without any extra code by executing the following command:\n",
    "```bash\n",
    "mlflow models serve -m ./mlruns/0/b9ee36e80a934cef9cac3a0513db515c/artifacts/model_random_forest/\n",
    "```\n",
    "You have access to a very simple web server that can run your model. Your model prediction interface can be executed by running the following command:\n",
    "```bash\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\"data\":[[1,1,1,1,0,1,1,1,0,1,1,1,0,0]]}' [1]%\n",
    "```\n",
    "The response to the API call to our model was 1; as defined in our predicted variable, this means that in the next reading, the stock will move up.\n",
    "\n",
    "The final few steps outline how powerful MLflow is as an end-to-end tool for model development, including for the prototyping of REST-based APIs for ML services.\n",
    "\n",
    "The MLflow Models component allows the creation of custom-made Python modules that will have the same benefits as the built-in models, as long as a prediction interface is followed.\n",
    "\n",
    "Some of the notable model types supported will be explored in upcoming chapters, including the following:\n",
    "\n",
    "* XGBoost model format\n",
    "* R functions\n",
    "* H2O model\n",
    "* Keras\n",
    "* PyTorch\n",
    "* Sklearn\n",
    "* Spark MLib\n",
    "* TensorFlow\n",
    "* Fastai\n",
    "\n",
    "Support for the most prevalent ML types of models, combined with its built-in capability for on-premises and cloud deployment, is one of the strongest features of MLflow Models. We will explore this in more detail in the deployment-related chapters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring MLflow Model Registry**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model registry component in MLflow **gives the ML developer an abstraction for model life cycle management**. It is a centralized store for an organization or function that allows models in the organization to be shared, created, and archived collaboratively.\n",
    "\n",
    "The management of the model can be made with the different APIs of MLflow and with the UI. Upon registering the model, you can annotate the registered model with the relevant metadata and manage its life cycle. One example is to have models in a staging pre-production environment and manage the life cycle by sending the model to production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct  7 2022, 20:14:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
